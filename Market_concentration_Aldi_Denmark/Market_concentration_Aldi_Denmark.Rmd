---
title: 'Introduction to Data Science - Winter 2019'
author: 'Mads Emil Marker Jungersen, studienummer: 201906249'
date: 'Dec 05, 2019'
output:
  pdf_document: default
  html_document:
    theme: readable
subtitle: 'End-term assignment'
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(ggplot2)
library(rstudioapi)    
library(stringr)
library(googleway)
library(ggmap)
library(geosphere)
library(knitr)
library(kableExtra)
library(cowplot)
library(reshape2)
# theme_set(theme_cowplot())
keyD <- "Insert Key"

source("Input_for_report/Functions/my_functions_v01.R")
```

# Introduction

In this report we will analyse the market concentration af Aldi stores in certain areas in Denmark. The main focus will be a description of the dataset and the explanatory variables with the use of descriptive tools as well as graphically illustrations showing how the different variables are distributed in different areas of Denmark. Furthermore the report will contain a description of the linear relationship between the explanatory variables in the dataset using correlation-matrices and scatter plots. In the end the report touches areas of linear regression trying to build a simple linear model that best explains/fits our data. Doing that we will explain terms as the R-squarred and the adjusted R-squarred. We will later use that linear model to see how well it predicts the number of Aldi-stores in a given area, for instance by calculating the average model error/residual. Finally we will split up our dataset in what we call a "Training dataset" and a "Test dataset" finding the best linear model on our Training dataset and use that for predictions on the Test dataset (out of sample). To test how well our linear model do, we will manually calculate the R-squarred. 

# Data description

## Distribution of Aldi Stores in Denmark

First of all we are going to load the dataset 'ALDI.rds' which we made in week 8-9 and contains all Aldi Stores in Denmark. We made that dataset using the 'google_maps' api and the 'google_places()' function.

```{r import datafile ALDI.rds}
Aldi <- readRDS(file = "Input_for_report/ALDI.rds")
Aldi %>%
  head() %>%
  kable("latex", booktabs = T, digits = 3, 
        col.names = c("Name", "Address", "Lat", "Lng"))%>%
  kable_styling(latex_options = c("striped"))
```


As we see the dataset contains some stores in Germany, as we are not intereseted in. Because all addresses contains the country of the store, we can use the 'string_detect()' togehter with the 'filter()' function to select only stores in Denmark.

\pagebreak

```{r Filtering out only Aldi in DK}
Aldi <- Aldi %>%
  filter(str_detect(address, 'Denmark'))

Aldi %>%
  head() %>%
  kable("latex", booktabs = T, digits = 3, 
        col.names = c("Name", "Address", "Lat", "Lng"))%>%
  kable_styling(latex_options = c("striped"))
```

Now that we have a dataset only containing Aldi stores in Denmark, we are going to look how the Aldi stores are distributed across Denmark. To do that, we create a plot using the 'get_map()' function to get a view of Denmark, and then plot the seperate Aldi stores on the map. Furthermore we are going to select a Aldi-store as centrum for our map.

 

```{r DK Plot, message=FALSE}
register_google(key = keyD)

# Use store in Gothersgade Copenhagen as centrum
use <- which(str_detect(Aldi$address, "Gothersgade"))

# Generate points of the Aldi-stores to plot on our map.
spec_point    <- Aldi[use,]
other_points  <- Aldi[-use,]

# Save our map with centrum in Gothersgade.
spec_map <- get_map(location = c(lon = Aldi$lng[use], lat = Aldi$lat[use]), zoom = 6, 
                    scale = 4, maptype ='terrain', color = 'color')

# Plotting the stores on the map, and save it in the variable 'mapPoints'.
mapPoints <- ggmap(spec_map) + 
  geom_point(aes(x = lng, y = lat), data=other_points, alpha=0.7, size = 1) +
  geom_point(aes(x = lng, y = lat), data=spec_point, alpha=0.7, size = 2) +
  labs (title = "Aldi Stores in Denmark")+
  theme(axis.title.x=element_blank(), axis.text.x=element_blank(), 
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(), axis.text.y=element_blank(), 
        axis.ticks.y=element_blank(),
        plot.title = element_text(hjust = 0.5),
        legend.position="right",
        axis.title=element_text(size=18,face="bold")
  ) +
  NULL

plot(mapPoints)

```

As we see the Aldi stores are more or less evenly distributed across Denmark, with some clusters around the greather cities as Aarhus and Copenhagen and in "Trekantsområdet". We are now going to take a closer look to how the Aldi stores are distributed in Aarhus and Copenhagen. We already have Copenhagen as centrum for our map, so we'll start here. 

## Distribution of Aldi Stores in Copenhagen

We are more or less going to reuse the previous plot, where we change the zoom attribute. Furthermore we are going to create 3 circles with the 'make_circles()' function, which we have defined in '/input_for_report/Functions/my_functions_v01.R'. The function takes in the Address, Latitude and Longitude as well as the desired Radius as input. The centrum in our case is the Aldi store in Gothersgade and we are going to use 2,5,10 km as radiuses.


```{r CPH Plot, message=FALSE, warning=FALSE}

# Store in Gothersgade as centrum
use <- which(str_detect(Aldi$address, "Gothersgade"))

# Create circles with radius 2, 5 and 10 km
myCircles5  <- make_circles(Aldi[,c(2:4)], 5)
myCircles10 <- make_circles(Aldi[,c(2:4)], 10)
myCircles2 <- make_circles(Aldi[,c(2:4)], 2)

# Create circles with centrum in the store in Gothersgade
spec_circle5  <- myCircles5  %>% filter(ID==Aldi$address[use]) 
spec_circle10 <- myCircles10 %>% filter(ID==Aldi$address[use]) 
spec_circle2 <- myCircles2 %>% filter(ID==Aldi$address[use]) 

# Save our map with centrum in Gothersgade.
spec_map_cph <- get_map(location = c(lon = Aldi$lng[use], lat = Aldi$lat[use]), zoom = 11, 
                    scale = 4, maptype ='terrain', color = 'color')

# Plotting the stores on the map, and save it in the variable 'mapPoints_cph'.
mapPoints_cph <- ggmap(spec_map_cph) + 
  geom_point(aes(x = lng, y = lat), data=other_points, alpha=0.7, size = 3) +
  geom_point(aes(x = lng, y = lat), data=spec_point, alpha=0.7, size = 5) +
  geom_polygon(data = spec_circle5, aes(lon, lat, group = ID), color = "red", alpha = 0) +
  geom_polygon(data = spec_circle10, aes(lon, lat, group = ID), color = "red", alpha = 0) +
  geom_polygon(data = spec_circle2, aes(lon, lat, group = ID), color = "red", alpha = 0) +
  labs (title = "Aldi Stores in Copenhagen")+
  theme(axis.title.x=element_blank(), axis.text.x=element_blank(), 
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(), axis.text.y=element_blank(), 
        axis.ticks.y=element_blank(),
        plot.title = element_text(hjust = 0.5),
        legend.position="right",
        axis.title=element_text(size=18,face="bold")
  ) +
  NULL

plot(mapPoints_cph)
```

As we can see from the plot, the Aldi-store in Gothersgade is the only Aldi store within a radius of 2 km. Furthermore there is 10 other Aldi stores within a radius of 5 km from the Aldi store in Gothersgade. We also see that almost every Aldi store in the Copenhagen area (except 4 stores) is within the radius of 10 km of the store in Gothersgade. In total we have 25 Aldi-stores in the Copenhagen Area, whereas $\frac{11}{25}\approx 44\%$ are within the radius of 5 km of the one in Gothersgade. 

Because there is a limited number of Aldi-stores in Copenhagen (compared to for instance Netto) we were able to count the Stores within the different radiuses, but in the case where the number of stores gets to high, we can generate a tibble containing the crow flight distances from the store we use as centrum to all the other stores. Here we are going to use the 'distm()' function in a for loop:

```{r}
# Creating an empty list with nrow(Aldi)-elements.
All_distance <- vector("list", nrow(Aldi))

# Creating a for loop, looping through all coordinates for the Aldi stores
for (p in 1:nrow(Aldi)){
  temp_res <- distm(c(Aldi$lng[use], Aldi$lat[use]), c(Aldi$lng[p],Aldi$lat[p]))/1000
  
  # Save results in a tibble
  res <- tibble(
    Centrum = Aldi$address[use],
    Destination = Aldi$address[p],
    Distance = temp_res[1]
    
  )
  # Save our tibble in our list
  All_distance[[p]] <- res
}


## We then combine the entries of our list into one tibble, 
## by iteratively taking the union with the next entry of the list

Final_distances_cph <- All_distance[1][[1]]


for (p in 2:length(All_distance)){
  Final_distances_cph <- union(Final_distances_cph,All_distance[p][[1]])
}

## What does our tibble look like
Final_distances_cph %>% 
  head() %>%
  kable("latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))

```

With that dataset we can simply use the 'filter()' function and the 'nrow()' function to find the amount of stores within a 10 km radius of centrum:

```{r}
Final_distances_cph %>%
  filter(Distance <= 10) %>%
  nrow()
```

So there is 21 Aldi stores within a 10km radius (crow flight) of our centrum. The above function might not feel necessary, but becomes very handy and easily to change to, for instance, finding the distance to different nearest competitors, which is (unfortunately) not within the scope of this assignment. As for now, we are going to take a closer look on how the Aldi stores are distributed in Aarhus. 

## Distribution of Aldi Stores in Aarhus

We are going to reuse the previous plot for Copenhagen only changing the centrum Store to the Aldi store in 'Grønnegade' in Aarhus, so in that sense we are not going to show the code generating the plot:

```{r AAH PLOT, message=FALSE, warning=FALSE, echo=FALSE}

# store in 'Grønnegade' as centrum
use_aah <- which(str_detect(Aldi$address, "Grønnegade"))

# Create circles with radius 2, 5 and 10 km
myCircles5  <- make_circles(Aldi[,c(2:4)], 5)
myCircles10 <- make_circles(Aldi[,c(2:4)], 10)
myCircles2 <- make_circles(Aldi[,c(2:4)], 2)

# Create circles with centrum in the store in 'Grønnegade'.
spec_circle5  <- myCircles5  %>% filter(ID==Aldi$address[use_aah]) 
spec_circle10 <- myCircles10 %>% filter(ID==Aldi$address[use_aah]) 
spec_circle2 <- myCircles2 %>% filter(ID==Aldi$address[use_aah]) 


# Save our map with centrum in'Grønnegade', using get_map().
spec_map_aah <- get_map(location = c(lon = Aldi$lng[use_aah], lat = Aldi$lat[use_aah]), zoom = 11, 
                    scale = 4, maptype ='terrain', color = 'color')

# Plotting the stores on the map, and save it in the variable 'mapPoints_aah'.
mapPoints_aah <- ggmap(spec_map_aah) + 
  geom_point(aes(x = lng, y = lat), data=other_points, alpha=0.7, size = 3) +
  geom_point(aes(x = lng, y = lat), data=spec_point, alpha=0.7, size = 5) +
  geom_polygon(data = spec_circle5, aes(lon, lat, group = ID), color = "red", alpha = 0) +
  geom_polygon(data = spec_circle10, aes(lon, lat, group = ID), color = "red", alpha = 0) +
  geom_polygon(data = spec_circle2, aes(lon, lat, group = ID), color = "red", alpha = 0) +
  labs (title = "Aldi in Aarhus")+
  theme(axis.title.x=element_blank(), axis.text.x=element_blank(), 
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(), axis.text.y=element_blank(), 
        axis.ticks.y=element_blank(),
        plot.title = element_text(hjust = 0.5),
        legend.position="right",
        axis.title=element_text(size=18,face="bold")
  ) +
  NULL

plot(mapPoints_aah)

```

As we se on the plot there are a total of 11 Aldi Stores in Aarhus, whereas 1 is within a radius of 2 km of our centrum store, 4 more Aldi stores are within a radius of 5 km of our centrum store and the last 5 stores are more than 5 km in crow flight distance away from our centrum store. Out of the total amount of 11 Aldi stores we have $\frac{6}{11}\approx 55\%$ of them within a radius of 5 km to our centrum store, which is around $10\%$-points more than in Copenhagen. Because of the relatively small amount of Aldi-strores in Aarhus, it would be a litle to comprehensive to implement the function callculating the distances as we did for the Copenhagen case. 

## Number of Aldi Stores pr. 10.000 capita in different areas

We are now going to evaluate the number of Aldi stores pr. Capita in the 80 areas of Denmark defined in the Dataset 'Discount_Concentration.rds'. We made that Dataset by extracting data from 'Danmarks Statistik' (DST) using their API, and adding it to the dataset containing our discount stores. The R-scripts doing so are in the R_Scripts folder within this zip-file, where the R-script called '3_Add_DST_data_v01.r' are the one merging all the different datasets to the desired dataset 'Discount_Concentration.rds'. We are now going to load in the dataset 'Discount_Concentration.rds'.

```{r Load in dataset containg DST and discout stores}
Disc_conc <- readRDS(file = "Input_for_report/Discount_Concentration.rds")
```

Because we are only interested in the Aldi stores, we are going to sort out the other discount stores, using the 'select()' function. Furthermore in this case we are only intereseted in the number of stores per capita in the different areas, so we select the columns of interest and using the 'Mutate()' function to generete the number of Aldi-stores pr 10.000 capita. 

```{r Making dataset for stores per cap}
Disc_conc_Aldi_Count <- Disc_conc %>%
  mutate(store_pr_cap = (ALDI/Total)*10000)%>%
  select(c(ALDI, Område, Region, Total, store_pr_cap))%>%
  arrange(desc(store_pr_cap))


Disc_conc_Aldi_Count %>%
  head()%>%
  kable(col.names = c("Nr. Aldi Stores", "Area", "Region", 
                      "Population", "Stores pr. Cap."),
        digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))

```

It is import to mention that our dataset only contains areas in Denmark where all of the 4 discount stores Netto, Fakta, Aldi, Rema 1000 and Lidl are present.  That being said, the above table show the top 6 areas of our dataset containing the most Aldi Stores per 10.000 capita. Brøndby is the city containg the most stores per capita, with around 1.7 Aldi stores pr 10.000 capita. The below table show the 6 areas in dataset containg least Aldi stores pr. capita. It shows that Randers with around 0.1 Aldi stores pr. 10.000 capita is the city in our dataset with least Aldi stores pr. capita. 

```{r Tail of stores per cap, echo=FALSE}
Disc_conc_Aldi_Count %>%
  tail()%>%
  kable(col.names = c("Nr. Aldi Stores", "Area", "Region", "Population", "Stores pr. Cap."),
        digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))
```

To get a feeling of how the number of Aldi stores pr 10.000 capita is distributed across all 80 areas in our dataset, we can call the 'summary()' function:

```{r summary of stores pr cap}

Store_pr_cap_sum <- t(as.data.frame.complex(summary(Disc_conc_Aldi_Count$store_pr_cap)))

rownames(Store_pr_cap_sum)<- c("Store pr. 10.000 capita")
Store_pr_cap_sum %>%
  kable(digits = 3, "latex", booktabs = T)%>%
  kable_styling()
```

To shortly conclude on the table, it gives us the minimum amount of Aldi stores pr. 10.000 capita (In Randers), the maximum amount (in Brøndby), but also the 1st and 3rd quantile and the mean and median of the number of Aldi stores pr. 10.000 capita. Across the 80 areas there are around 0.7 Aldi stores pr. 10.000 capita on average. To change our scope a bit, we can also see which of the 5 regions in our dataset has the highest amount of Aldi stores pr. cap and how the distribution is. Below is bar-plot and a box-plot containg this information.

```{r plotgrid for stores pr cap, message=FALSE, warning=FALSE}
Count_bar <- Disc_conc_Aldi_Count %>%
  group_by(Region) %>%
  summarise(Store_pr_cap_reg = sum(ALDI)*10000/sum(Total)) %>%
  ggplot(mapping = aes(x= Region, y= Store_pr_cap_reg))+
  geom_col(stat="identity")+
  labs(y = "Stores pr. 10.000 cap pr. region",
       title = "Nr. of stores pr. cap pr. region")+
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10, angle=30, vjust=.8, hjust=0.8),
        plot.title = element_text(size=11, hjust = 0.5))
  


Count_box <- Disc_conc_Aldi_Count %>%
  ggplot(mapping = aes(x= Region, y= store_pr_cap))+
  geom_boxplot()+
  labs(y = "Stores pr. 10.000 cap",
       title = "Distribution of Nr. of stores pr. region")+
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10, angle=30, vjust=.8, hjust=0.8),
        plot.title = element_text(size=11, hjust = 0.5))

plot_grid(Count_bar,Count_box, nrow = 1)
```

The above bar plot shows that pr. region, Region Sjælland has the higest number of stores pr 10.000 capita with around 0.8 stores pr. 10.000 capita. Region Nordjylland has the second highest number of Aldi stores pr. capita with around 0.7 Aldi stoers pr. capita. The three remaining regions have around 0.5 aldi stores pr. 10.000 capita. 

The boxplot however reveals that within each region there is some variation on the distribution of Aldi stores pr. capita, especially in Region Hovedstaden and Mdtjylland ranging from around $0.12$ to $1.7$ stores pr. 10.000 capita. Region Syddanmark is the region with the lowest variation ranging from around $0.20$ to around $0.80$ stores pr. 10.000 capita. 

## Description of explanatory variables in the dataset and how they varies in the different areas

This section will contain a short description of each of the explanatory variables in the dataset, with plot and tables to see how the variables are distributed across the regions/areas.

### Kvinder

The explanatory variable 'Kvinder' contains the number of women in the area in the 3rd quarter of 2019. This explanatory variable is collected in the '2_Get_population_FOLK1A_v01.r' R-script using the DST-Api more precisely the 'dst_meta()' and 'dst_get_data()'functions. The amount of women dosen't say much, and it is probably more insightfull to look at the ratio of women, so we create a new column containg the ratio of women, therenext finding the 5 areas with the highest women ratio and the 5 areas with the lowest women ratio, aswell as a bar-plot and a boxplot containing the women ratio for the 5 regions and the distribution of the women ratio in the 5 regions. In the end we use 'summary()' function to see the distribution of the women ratio across all 80 areas. The tables and plot is done in the same way as we did in the analysis of the number of stores pr. cap, so in that sense we will not print the code:

```{r plotgrid for women ratio, message=FALSE, warning=FALSE, echo=FALSE}
Disc_conc$Women_ratio <- Disc_conc$Kvinder/Disc_conc$Total 

Disc_conc %>% select(c(ALDI, Område, Region, Total, Women_ratio))%>%
  arrange(desc(Women_ratio)) %>%
  head() %>%
  kable(col.names = c("Nr. Aldi Stores", "Area", "Region", "Population", "Women Ratio"),
        digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))

Disc_conc %>% select(c(ALDI, Område, Region, Total, Women_ratio))%>%
  arrange(desc(Women_ratio)) %>%
  tail() %>%
  kable(col.names = c("Nr. Aldi Stores", "Area", "Region", "Population", "Women Ratio"),
        digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))

Women_bar <- Disc_conc %>%
  group_by(Region) %>%
  summarise(Women_ratio_reg = sum(Kvinder)/sum(Total)) %>%
  ggplot(mapping = aes(x= Region, y= Women_ratio_reg))+
  geom_col(stat="identity")+
  labs(y = "Women Ratio pr. region",
       title = "Women ratio pr. region")+
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10, angle=30, vjust=.8, hjust=0.8),
        plot.title = element_text(size=11, hjust = 0.5))
  


Women_box <- Disc_conc %>%
  ggplot(mapping = aes(x= Region, y= Women_ratio))+
  geom_boxplot()+
  labs(y = "Women Ratio",
       title = "Distribution of Women Ratio pr. region")+
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10, angle=30, vjust=.8, hjust=0.8),
        plot.title = element_text(size=11, hjust = 0.5))

plot_grid(Women_bar,Women_box, nrow = 1)


wom_sum <- t(as.data.frame.complex(summary(Disc_conc$Women_ratio)))

rownames(wom_sum)<- c("Women Ratio")
wom_sum %>%
  kable(digits = 3, "latex", booktabs = T)%>%
  kable_styling()

```

As the above tables and plotes reveal the women ratio seems quite evenly distributed across the 80 areas. The lowest women ratio of around $48.7\%$ is in Norddjurs, while the highest women ratio of around $52.5\%$ is in Frederiksberg. The overall median and mean is around $50\%$. Eventhough the scatterplot might show some variation within each region, the scale on the y-axis is low, so actually the variation is quite low as well.   

### Mænd 

The explanatory variable 'Mænd' contains the number of men in the area in the 3rd quarter of 2019. This explanatory variable is collected in the '2_Get_population_FOLK1A_v01.r' R-script using the DST-Api more precisely the 'dst_meta()' and 'dst_get_data()'functions. We chose again to look at the ratio of men. We create a new column containg the ratio of men, therenext finding the 5 areas with the highest men ratio and the 5 areas with the lowest men ratio, aswell as a bar-plot and a boxplot containing the men ratio for the 5 regions and the distribution of the men ratio in the 5 regions. In the end we use 'summary()' function to see the distribution of the men ratio across all 80 areas: 

```{r plotgrid for men ratio, message=FALSE, warning=FALSE, echo=FALSE}
Disc_conc$Men_ratio <- Disc_conc$Mænd/Disc_conc$Total 

Disc_conc %>% select(c(ALDI, Område, Region, Total, Men_ratio))%>%
  arrange(desc(Men_ratio)) %>%
  head() %>%
  kable(col.names = c("Nr. Aldi Stores", "Area", "Region", "Population", "Men Ratio"),
        digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))

Disc_conc %>% select(c(ALDI, Område, Region, Total, Men_ratio))%>%
  arrange(desc(Men_ratio)) %>%
  tail() %>%
  kable(col.names = c("Nr. Aldi Stores", "Area", "Region", "Population", "Men Ratio"),
        digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))

Men_bar <- Disc_conc %>%
  group_by(Region) %>%
  summarise(Men_ratio_reg = sum(Mænd)/sum(Total)) %>%
  ggplot(mapping = aes(x= Region, y= Men_ratio_reg))+
  geom_col(stat="identity")+
  labs(y = "Men Ratio pr. region",
       title = "Men ratio pr. region")+
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10, angle=30, vjust=.8, hjust=0.8),
        plot.title = element_text(size=11, hjust = 0.5))
  


Men_box <- Disc_conc %>%
  ggplot(mapping = aes(x= Region, y= Men_ratio))+
  geom_boxplot()+
  labs(y = "Men Ratio",
       title = "Distribution of Men Ratio pr. region")+
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10, angle=30, vjust=.8, hjust=0.8),
        plot.title = element_text(size=11, hjust = 0.5))

plot_grid(Men_bar,Men_box, nrow = 1)


men_sum <- t(as.data.frame.complex(summary(Disc_conc$Men_ratio)))

rownames(men_sum)<- c("Men Ratio")
men_sum %>%
  kable(digits = 3, "latex", booktabs = T)%>%
  kable_styling()

```

The above plot and tables show more or less the same as the Women Ratio, which makes sense because the number of men and the number of women represent the total amount of people living in the are. 

### Danmark

The variable 'Danmark' is the total number of danish citizens living in the area in the 3rd quarter of 2019. It is collected in the '2_Get_population_FOLK1B_v01.r' using the same method as the previous two variables. As we did before we are going to find the ratio of danish citizens in the area, but because the variable is chosen to be less important also due to the next variable, we are only doing a 'summary()' to get a distribution of the ratio across the 80 areas:

```{r summary Danish citizen ratio, message=FALSE, warning=FALSE, echo=FALSE}
Disc_conc$Danish_cit_ratio <- Disc_conc$Danmark/Disc_conc$Total


da_ci_sum <- t(as.data.frame.complex(summary(Disc_conc$Danish_cit_ratio)))

rownames(da_ci_sum)<- c("Danish Citizen Ratio")
da_ci_sum %>%
  kable(digits = 3, "latex", booktabs = T)%>%
  kable_styling()

```

Looking at the summary table it seems that there is one or two outliers where the the Danish Citizen ratio is around $78\%$, while in the other areas it seems to be more stable around $92-95\%$.  

### Indvandrere

The explanatory variable 'Indvandrere' is collected in the '2_Get_population_FOLK1C_v01.r' R-script using the same method as the previous variables. It describes the number of immigrants living in the area in the 3rd quarter of 2019. As before we first calculate the immigrant ratio, then we show the 5 areas having the highest immigrant ratio and the 5 areas having the lowest immigrant ratio. We will also display the immigrant ratio pr. region and the dsitribution of the immigrant ratio pr. region by a bar- and boxplot:

```{r plotgrid for immigrant ratio, message=FALSE, warning=FALSE, echo=FALSE}
Disc_conc$Immigrant_ratio <- Disc_conc$Indvandrere/Disc_conc$Total 

Disc_conc %>% select(c(ALDI, Område, Region, Total, Immigrant_ratio))%>%
  arrange(desc(Immigrant_ratio)) %>%
  head() %>%
  kable(col.names = c("Nr. Aldi Stores", "Area", "Region", "Population", "Immigrant Ratio"),
        digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))

Disc_conc %>% select(c(ALDI, Område, Region, Total, Immigrant_ratio))%>%
  arrange(desc(Immigrant_ratio)) %>%
  tail() %>%
  kable(col.names = c("Nr. Aldi Stores", "Area", "Region", "Population", "Immigrant Ratio"),
        digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))

Immigrant_bar <- Disc_conc %>%
  group_by(Region) %>%
  summarise(Immigrant_ratio_reg = sum(Indvandrere)/sum(Total)) %>%
  ggplot(mapping = aes(x= Region, y= Immigrant_ratio_reg))+
  geom_col(stat="identity")+
  labs(y = "Immigrant Ratio pr. region",
       title = "Immigrant ratio pr. region")+
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10, angle=30, vjust=.8, hjust=0.8),
        plot.title = element_text(size=11, hjust = 0.5))
  


Immigrant_box <- Disc_conc %>%
  ggplot(mapping = aes(x= Region, y= Immigrant_ratio))+
  geom_boxplot()+
  labs(y = "Immigrant Ratio",
       title = "Distribution of Immigrant Ratio pr. region")+
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10, angle=30, vjust=.8, hjust=0.8),
        plot.title = element_text(size=11, hjust = 0.5))

plot_grid(Immigrant_bar,Immigrant_box, nrow = 1)


imm_sum <- t(as.data.frame.complex(summary(Disc_conc$Immigrant_ratio)))

rownames(imm_sum)<- c("Immigrant Ratio")
imm_sum %>%
  kable(digits = 3, "latex", booktabs = T)%>%
  kable_styling()

```

From the above plots and tables we see, that the highest Immigration Ratio on around $25\%$ is in Ishøj whereas the lowest immigration ratio is achived in Rebild, where the immigration ratio is around $5\%$. The overall mean in the 80 areas is around $10\%$, where region Hovedstaden on average have the highest immigration ratio. The boxplot also reveals that Region Hovedstaden has the highest variation of immigration ratio. While the distribution in the other regions are less spread out. 

### Dansk_oprindelse

The explanatory variable 'Dansk_oprindelse' is collected in the same R-script as the variable 'Indvadrere'. It describes the number of people with danish origin, living in the area in the 3rd quarter of 2019. We will again calculate the ratio of people with danish origin and use the 'summary()' function to see the distribution of the ratio across the 80 areas.


```{r summary Danish origin ratio, message=FALSE, warning=FALSE, echo=FALSE}
Disc_conc$Danish_origin_ratio <- Disc_conc$Dansk_oprindelse/Disc_conc$Total


da_or_sum <- t(as.data.frame.complex(summary(Disc_conc$Danish_origin_ratio)))

rownames(da_or_sum)<- c("Danish Origin")
da_or_sum %>%
  kable(digits = 3, "latex", booktabs = T)%>%
  kable_styling()

```

Looking at the above summary table, it seems that (which was also the case in Danish Citizen Ratio) there is one or two outliers with low Danish Origin Ratio, while in the other cases it seems to be rather stable around give or take $90\%$. 

### Alder

The explanatory variable 'Alder' is collected in the '2_Get_population_FOLK1A_alder_v01.r' and describes the average age in the area in the 3rd quarter of 2019. The distribution of the average age accross the 80 araes is again displayed using the 'summary()' function. 

```{r summary avg age, message=FALSE, warning=FALSE, echo=FALSE}

age_sum <- t(as.data.frame.complex(summary(Disc_conc$Alder)))

rownames(age_sum)<- c("Avg. Age")
age_sum %>%
  kable(digits = 3, "latex", booktabs = T)%>%
  kable_styling()

```

As we see in the above summary table the Avg. age across all 80 areas takes the minimum value of 35.5 years and the maximum value of around 47.7 years. The overall mean of the avg. Age is around 42 years.

### Indkomst

The explanatory variable 'Indkomst' is collected in the '2_Get_indkomst_v01.r' script, and it describes the average household income in 2018 in the Area. We are going to display the top 5 areas with the highest average household income as well as the 5 areas with the lowest average household income. We will also make a bar plot containing the average household income in each region, and also a boxplot displaying the distribution of the average household income in each region. We will also show the distribution across all 80 ares using the 'summary()' function:

```{r plotgrid for avg household income, message=FALSE, warning=FALSE, echo=FALSE}

Disc_conc %>% select(c(ALDI, Område, Region, Total, Indkomst))%>%
  arrange(desc(Indkomst)) %>%
  head() %>%
  kable(col.names = c("Nr. Aldi Stores", "Area", "Region", "Population", "Avg. Household Income"),
        digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))

Disc_conc %>% select(c(ALDI, Område, Region, Total, Indkomst))%>%
  arrange(desc(Indkomst)) %>%
  tail() %>%
  kable(col.names = c("Nr. Aldi Stores", "Area", "Region", "Population", "Avg. Household Income"),
        digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))

Income_bar <- Disc_conc %>%
  group_by(Region) %>%
  summarise(Income = sum(Indkomst)/n()) %>%
  ggplot(mapping = aes(x= Region, y= Income))+
  geom_col(stat="identity")+
  labs(y = "Avg. Household Income pr. region",
       title = "Income pr. region")+
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10, angle=30, vjust=.8, hjust=0.8),
        plot.title = element_text(size=11, hjust = 0.5))
  


Income_box <- Disc_conc %>%
  ggplot(mapping = aes(x= Region, y= Indkomst))+
  geom_boxplot()+
  labs(y = "Avg. Household Income",
       title = "Distribution of income pr. region")+
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10, angle=30, vjust=.8, hjust=0.8),
        plot.title = element_text(size=11, hjust = 0.5))

plot_grid(Income_bar,Income_box, nrow = 1)

inc_sum <- t(as.data.frame.complex(summary(Disc_conc$Indkomst)))

rownames(inc_sum)<- c("Avg. Household Income")
inc_sum %>%
  kable(digits = 3, "latex", booktabs = T)%>%
  kable_styling()

```

It is shown from the above tables and plots that the minimum average household income on around 280.000 is in Lolland, where the area with the highest average household income is Egedal with around 430.000. On average by region the household income seems to be evenly distributed, but within each region there is some kind of variation, especially in Region Hovedstaden. 

### Besk

The explanatory variable 'Besk' describes the employment rate in an area in 2018. The variable is collected in the '2_Get_beskæft_v01.r' script. We are now going to display the 5 ares with the highest employment rate, and the 5 ares with the lowest employment rate. Furthermore we will make a bar plot describing the average employment rate for each of the 5 regions, and a boxplot showing the distribution of the employment rate within each region. We will also display the overall distribution of the employment rate for all areas using the 'summary()' function:

```{r plotgrid for avg employment rate, message=FALSE, warning=FALSE, echo=FALSE}

Disc_conc %>% select(c(ALDI, Område, Region, Total, Besk))%>%
  arrange(desc(Besk)) %>%
  head() %>%
  kable(col.names = c("Nr. Aldi Stores", "Area", "Region", "Population", "Employment rate"),
        digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))

Disc_conc %>% select(c(ALDI, Område, Region, Total, Besk))%>%
  arrange(desc(Besk)) %>%
  tail() %>%
  kable(col.names = c("Nr. Aldi Stores", "Area", "Region", "Population", "Employment rate"),
        digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))

Employment_bar <- Disc_conc %>%
  group_by(Region) %>%
  summarise(Employment_rate = sum(Besk)/n()) %>%
  ggplot(mapping = aes(x= Region, y= Employment_rate))+
  geom_col(stat="identity")+
  labs(y = "Avg. Employment rate pr. region",
       title = "Employment rate pr. region")+
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10, angle=30, vjust=.8, hjust=0.8),
        plot.title = element_text(size=11, hjust = 0.5))
  


Employment_box <- Disc_conc %>%
  ggplot(mapping = aes(x= Region, y= Besk))+
  geom_boxplot()+
  labs(y = "Employment rate",
       title = "Distribution of Employment rate pr. region")+
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10, angle=30, vjust=.8, hjust=0.8),
        plot.title = element_text(size=11, hjust = 0.5))

plot_grid(Employment_bar,Employment_box, nrow = 1)


besk_sum <- t(as.data.frame.complex(summary(Disc_conc$Besk)))

rownames(besk_sum)<- c("Employment Rate")
besk_sum %>%
  kable(digits = 3, "latex", booktabs = T)%>%
  kable_styling()

```

Looking at the above tables and plots it is clear, that on average by region the Employment Rate seems to be evenly distributed on around $80\%$. However the boxplot reveals some variation within the regions, especially Region Sjælland, Syddanmark and Hovedstaden, wheras Region Midtjylland and Nordjylland seems more similar, but with some outliers. The employment rate overall goes from around $70%$ in Lolland to around $90\%$ in Allerød, where the overall mean is around $82\%$. 

### ArbStrk

The variable 'ArbStrk' describes the labor force in each area in the 3rd quarter of 2019. It is defined as people from age 18-65. It is collected in the '2_Get_population_FOLK1A_under18_v01.r' script. We will first calculate the labor force ratio and then use the 'summary()' function the display the distribution across the 80 areas.

```{r summary labor force ratio, message=FALSE, warning=FALSE, echo=FALSE}
Disc_conc$labor_force_ratio <- Disc_conc$ArbStrk/Disc_conc$Total

lab_sum <- t(as.data.frame.complex(summary(Disc_conc$labor_force_ratio)))

rownames(lab_sum)<- c("Labor Force")
lab_sum %>%
  kable(digits = 3, "latex", booktabs = T)%>%
  kable_styling()

```

The above summary table reveals that the overall mean of the Labor Force is around $40\%$ ranging from around $27\%$ as minimum value and $45\%$ as maximum value. 

# Describe the correlation between the number of Aldi stores pr. capita and the other explanatory variables

## Correlation matrix

In the following section we will create a table containg the correlation coefficient from the Aldi store pr. 10.000 capita to all the other explanatory variables. The correlation shows the strength and the direction of the linear relationship between the variables, compared to for instance the covariance which only gives the direction. The correlation coefficient ranges from -1 to 1, where the absolute value corresponds to the strength of the linear relationship i.e. the closer the coefficient is to -1 or 1 the stronger the linear relationship is. If the correlation coefficient is negative, it means that the two variables are negative correlated i.e if number of Aldi stores increases the other variable decreases. If the correlation coefficient is positive it shows a positive linear relationship i.e. when number of aldi stores increases the other variable increases as well. First we are going to show a graphical illustration of the correlation coefficient between all the variables. The below chart is a slightly changed version of the one found here: http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization, changed to fit our dataset.

```{r Matrix heatmap, echo=FALSE}
Disc_conc$Store_pr_cap <- (Disc_conc$ALDI/Disc_conc$Total)*10000
Aldi_corr <- Disc_conc %>%
  select(Alder:Store_pr_cap)

# Get upper triangle of the correlation matrix
  get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }

reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}
  
Aldi_cormatrix <- round(cor(Aldi_corr),2)
Aldi_cormatrix <- reorder_cormat(Aldi_cormatrix)
upper_tri <- get_upper_tri(Aldi_cormatrix)
melted_cormatrix <- melt(upper_tri, na.rm = TRUE)

ggheatmap <- ggplot(melted_cormatrix, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Correlation") +
  theme_minimal()+ # minimal theme
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()
# Print the heatmap
# print(ggheatmap)

ggheatmap + 
geom_text(aes(Var2, Var1, label = value), color = "black", size = 2) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))
```

The above chart shows how each varibale (icluding the number of Aldi stores pr. 10.000 capita) is correlated to each other. In this specific assignment we are mostly interested in how the number of Aldi stores pr. 10.000 capita is correlated with the other explanatory variables. However the mutual correlation between the explanatory variables may play a role when finding the best linear model to predict the data, but because the main scope of this assignment dosen't include deep knowledge of the linear regression, we will not dive into it here. For now we will collect the correlation coefficients of interest (between the number of Aldi stores to the other explanatory variables) and put them in a table/data frame. 

\pagebreak

```{r correlation coefficients}
Disc_conc$Store_pr_cap <- (Disc_conc$ALDI/Disc_conc$Total)*10000
Aldi_corr <- Disc_conc %>%
  select(Alder:Store_pr_cap)

as.data.frame(round(cor(Aldi_corr),3)[11,])%>%
  kable("latex", booktabs = T, col.names = c("Correlation from Store pr. cap"))%>%
  kable_styling()

```

Due to the fact that we will take a closer look on 4 chosen explanatory variables later in this assignment, the following section will create scatter plots for these specific variables displaying any correlation more graphically.  

## Scatter Plots

In this section we are going to make 4 scatter plots showing the correlation between the 4 explanatory variables Women Ratio, Immigrant Ratio, Income and Employment Rate and the number of Aldi stores. The code generating the plots are very similar, just changing the x-variable, so in that sense we'll only display the R code generating the first scatter plot. Each scatter plot contains a linear regrssion line to better visualise the correlation. 

```{r scatter plot income}
Disc_conc %>%
  ggplot(mapping=aes(x = Indkomst, y = Store_pr_cap))+
  geom_point(mapping = aes(color=Region))+
  geom_smooth(method=lm, se=F)+
  labs(title = "Avg. Household Income vs Nr. of Aldi stores pr. 10.000 cap",
       y= "Aldi Stores pr. 10.000 Cap.",
       x= "Avg. Household Income")+
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10),
        plot.title = element_text(size=11, hjust = 0.5))+
  NULL
```

As we can see from the above scatter plot, there is a slightly positive correlation between the average household income and the number of Aldi stores, as we also saw i the previous correlation matrix. However our data seems to be quite spread out, and the linear regression line dosent fit/explain our data very well. We will return to that later on, when we compute the R-squared value for the different linear regression models.

```{r scatter plot women_ratio, echo=FALSE}
Disc_conc %>%
  ggplot(mapping=aes(x = Women_ratio, y = Store_pr_cap))+
  geom_point(mapping = aes(color=Region))+
  geom_smooth(method=lm, se=F)+
  labs(title = "Women Ratio vs Nr. of Aldi stores pr. 10.000 cap",
       y= "Aldi Stores pr. 10.000 Cap.",
       x= "Women Ratio")+
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10),
        plot.title = element_text(size=11, hjust = 0.5))+
  NULL
```

Looking at the above scatter-plot it is clear that there is a negative correlation between the women ratio and the Aldi stores, that is when the number of Aldi stores increase the women ratio decreases and virsa versa. Again, the linear regression line dosen't seem to fit/explain our data very well.

```{r scatter plot Immigrant_ratio, echo=FALSE}
Disc_conc %>%
  ggplot(mapping=aes(x = Immigrant_ratio, y = Store_pr_cap))+
  geom_point(mapping = aes(color=Region))+
  geom_smooth(method=lm, se=F)+
  labs(title = "Immigrant Ratio vs Nr. of Aldi stores pr. 10.000 cap",
       y= "Aldi Stores pr. 10.000 Cap.",
       x= "Immigrant Ratio")+
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10),
        plot.title = element_text(size=11, hjust = 0.5))+
  NULL
```

By looking at the above plot, we see a weak negative correlation between the immigration ratio and the number of Aldi stores. As in the prevous cases the linear regression model doesent seem to fit/explain our data very well.

```{r scatter plot employment_rate, echo=FALSE}
Disc_conc %>%
  ggplot(mapping=aes(x = Besk, y = Store_pr_cap))+
  geom_point(mapping = aes(color=Region))+
  geom_smooth(method=lm, se=F)+
  labs(title = "Employment Rate vs Nr. of Aldi stores pr. 10.000 cap",
       y= "Aldi Stores pr. 10.000 Cap.",
       x= "Employment Rate")+
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10),
        plot.title = element_text(size=11, hjust = 0.5))+
  NULL
```

The last scatter-plot reveals a small positive correlation between the employment rate and the number of Aldi stores. Like in all of the previous scatter-plots the data is quite spread out, and the linear regression line therefore dosent fit our data to well.

# Models for prediction (Nr of Aldi stores pr. 10.000 capita)

In this section we are going to build a linear model to predict the number of Aldi Stores pr. 10.000 capita using the following 4 explanatory variables:

* Women Ratio
* Immigrant Ratio
* Avg. household income
* Employment Rate


## Computing the best model based on Adjusted R Squared

We are going to make 15 different linear models trying out different combinations of the 4 explanatory variables. We will be using the Adjusted R-Squared to determine wich of the models best describes our data. The R-squarred value usually goes from 0 to 1 telling how well the linear model describes/explains our data. If the R-Squarred value is one, then the linear model is able to explain $100\%$ of our data. When adding more explanatory variables to our model, the R-Squarred value will increase, but does that mean that our model gets better? Not necessarily, when adding more explanatory variables to our model, the model becomes less "flexible" due to loss of degrees of freedom. The Adjusted R-Squarred value takes the loss of degrees of freedom into account, which is why all else equal that the Adjusted R-squarred value in this case would be better to determine the best model. 

Below we have made the first model, which is simply just what we saw in the first scatterplot. The coefficints for the linear model and the R-squarred as well as the Adjusted R-Squarred values are saved in a list: 

```{r single model}
y <- Disc_conc$Store_pr_cap
x1 <- Disc_conc$Women_ratio
x2 <- Disc_conc$Immigrant_ratio
x3 <- Disc_conc$Indkomst
x4 <- Disc_conc$Besk

res <- summary(mod <- lm(y ~ x1))
m1  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )
```

The rest of the models are made in the same way, in the end we can combine the results for each model in a data.frame:

```{r computing best model for full dataset, echo=FALSE}
y <- Disc_conc$Store_pr_cap
x1 <- Disc_conc$Women_ratio
x2 <- Disc_conc$Immigrant_ratio
x3 <- Disc_conc$Indkomst
x4 <- Disc_conc$Besk


##----------------------Creating Models ------------------------

res <- summary(mod <- lm(y ~ x1))
m1  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y ~ x2))
m2  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y ~ x3))
m3  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y ~ x4))
m4  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y ~ x1+x2))
m5  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y ~ x1+x3))
m6  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y ~ x1+x4))
m7  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y ~ x2+x3))
m8  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y ~ x2+x4))
m9  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y ~ x3+x4))
m10  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y ~ x1+x2+x3))
m11  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y ~ x1+x2+x4))
m12  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y ~ x1+x3+x4))
m13  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y ~ x2+x3+x4))
m14  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y ~ x1+x2+x3+x4))
m15  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

Models <- data.frame(
          M1 = c(m1$coef,NA,NA,NA,m1$R2,m1$adj.R2), 
          M2 = c(m2$coef[1],NA,m2$coef[2],NA,NA,m2$R2,m2$adj.R2), 
          M3 = c(m3$coef[1],NA,NA,m3$coef[2],NA,m3$R2,m3$adj.R2), 
          M4 = c(m4$coef[1],NA,NA,NA,m4$coef[2],m4$R2,m4$adj.R2), 
          M5 = c(m5$coef[1:3],NA,NA,m5$R2,m5$adj.R2), 
          M6 = c(m5$coef[1:2],NA,m5$coef[3],NA,m6$R2,m6$adj.R2), 
          M7 = c(m7$coef[1:2],NA,NA,m7$coef[3],m7$R2,m7$adj.R2),
          M8 = c(m8$coef[1],NA,m8$coef[2:3],NA,m8$R2,m8$adj.R2),
          M9 = c(m9$coef[1],NA,m9$coef[2],NA,m9$coef[3],m9$R2,m9$adj.R2),
          M10 = c(m10$coef[1],NA,NA,m10$coef[2:3],m10$R2,m10$adj.R2),
          M11 = c(m11$coef[1:4],NA,m11$R2,m11$adj.R2),
          M12 = c(m12$coef[1:3],NA,m12$coef[4],m12$R2,m12$adj.R2),
          M13 = c(m13$coef[1:2],NA,m13$coef[3:4],m13$R2,m13$adj.R2),
          M14 = c(m14$coef[1],NA,m14$coef[2:4],m14$R2,m14$adj.R2),
          M15 = c(m15$coef,m15$R2,m15$adj.R2)
          )

rownames(Models) <- c("Intercept", "Women Ratio", "Immigrant Ratio", "Income", "Employment Rate", "R-squared", "adj.R-squared")


Models %>%
  kable(digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped", "scale_down"))%>%
  add_header_above(c("Computing linear models for full Dataset"=16))%>%
  column_spec(14, bold = T, border_left = T, border_right = T)

```

Based on the above table, we choose Model 13 as our best model, using the 3 explanatory variables Women Ratio, Income and Employment Rate. Eventhough model 15 has a higher R-Squarred value (using all 4 explanatory variables) we see that the variable Immigration Ratio dosent add significant value to our model based on loss of degrees of freedom expressed in the Adj. R-Squarred value. 

## Calculate the mean model-error for the Regions

In this section we are going to calculate the mean model-error for each region, displaying how well our model explains/predicts the number of Aldi stores in each region.

```{r mean model error for best model full data, message=FALSE, warning=FALSE}
# Best model (Model 13)
Best_model <- lm(y ~ x1+x3+x4)

# Create columns computing predictions and residuals
Disc_conc$Model_prediction <- predict(Best_model)
Disc_conc$Model_residual <- residuals(Best_model)
Disc_conc$Model_residual1 <- (Disc_conc$Store_pr_cap - Disc_conc$Model_prediction)

#Create a plot displaying mean model residuals grouped by region
Disc_conc %>%
  group_by(Region)%>%
  summarise(Avg_model_residual = mean(Model_residual))%>%
  mutate(Negative = Avg_model_residual < 0)%>%
  ggplot(mapping = aes(x= Region, y=Avg_model_residual, fill=Negative))+
  geom_col(stat="identity")+
  labs(y = "Avg. Model residuals",
       title = "Avg. Model residuals pr Region")+
  scale_fill_manual(values = c("green","red"))+ 
  theme_light()+
  theme(axis.text.x=element_text(color = "black", size=10, angle=30, vjust=.8, hjust=0.8),
        plot.title = element_text(size=11, hjust = 0.5),
        legend.position = "none")
  
```


We made the above plot by first creating 3 new columns in our dataset, where the column Model_prediction is the predicted number of Aldi stores pr 10.000 capita based on our best model. The second two columns show the same thing using to different methods. The first column 'Model_residual' is computed using the residuals() function, while the second column 'Model_residual1' is calculated based on our observed number of aldi stores pr. 10.000 capita -(minus) the predicted value. The two columns is identical. The plot then group or dataset by region and calculate the mean model error for each region. If the mean model error is negative it has the color red, while positive model errors are colored green. 

The above plot reveals that Region Syddanmark obtains the highest mean residual on around -0.25. That is, on average, our model predict 0.25 Aldi Stores pr. 10.000 capita more than what we observed in Region Syddanmark. Our model also overestimate the number of Aldi Stores in Region Midtjylland, while in the other regions our model underestimate the observed number of Aldi stores pr. 10.000 capita. Our model seems to best predict the number of Aldi stores in Region Hovedstanden where the mean residual is around 0.06. The exact Avg. Model residual for each region is displayed in the below table.

```{r mean model-error for syddanmark, echo=FALSE}
Disc_conc %>%
  group_by(Region)%>%
  summarise(Avg_model_residual = mean(Model_residual)) %>%
  kable(digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))
```

\pagebreak

## Repeating the analasis without Region Syddanmark

The below analysis finding the best model for the dataset without Region syddanmark follows the exactly same procedure as before. In this case we first split up our full dataset in a dataset called 'Training_data' where we have filtered Region Syddanmark out, and 'Test_data' wich is a dataset only containing values from Region Syddanmark.

```{r}
Training_data <- Disc_conc %>%
  filter(Region != "Syddanmark")

Test_data <- Disc_conc %>%
  filter(Region == "Syddanmark")

y_1 <- Training_data$Store_pr_cap
x1_1 <- Training_data$Women_ratio
x2_1 <- Training_data$Immigrant_ratio
x3_1 <- Training_data$Indkomst
x4_1 <- Training_data$Besk
```

We then combine all the different models in a dataframe:



```{r repeating analysis without syddanmark, echo=FALSE}

##----------------------Creating Models ------------------------

res <- summary(mod <- lm(y_1 ~ x1_1))
m1  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y_1 ~ x2_1))
m2  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y_1 ~ x3_1))
m3  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y_1 ~ x4_1))
m4  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y_1 ~ x1_1+x2_1))
m5  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y_1 ~ x1_1+x3_1))
m6  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y_1 ~ x1_1+x4_1))
m7  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y_1 ~ x2_1+x3_1))
m8  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y_1 ~ x2_1+x4_1))
m9  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y_1 ~ x3_1+x4_1))
m10  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y_1 ~ x1_1+x2_1+x3_1))
m11  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y_1 ~ x1_1+x2_1+x4_1))
m12  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y_1 ~ x1_1+x3_1+x4_1))
m13  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y_1 ~ x2_1+x3_1+x4_1))
m14  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

res <- summary(mod <- lm(y_1 ~ x1_1+x2_1+x3_1+x4_1))
m15  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

Models_training <- data.frame(
          M1 = c(m1$coef,NA,NA,NA,m1$R2,m1$adj.R2), 
          M2 = c(m2$coef[1],NA,m2$coef[2],NA,NA,m2$R2,m2$adj.R2), 
          M3 = c(m3$coef[1],NA,NA,m3$coef[2],NA,m3$R2,m3$adj.R2), 
          M4 = c(m4$coef[1],NA,NA,NA,m4$coef[2],m4$R2,m4$adj.R2), 
          M5 = c(m5$coef[1:3],NA,NA,m5$R2,m5$adj.R2), 
          M6 = c(m5$coef[1:2],NA,m5$coef[3],NA,m6$R2,m6$adj.R2), 
          M7 = c(m7$coef[1:2],NA,NA,m7$coef[3],m7$R2,m7$adj.R2),
          M8 = c(m8$coef[1],NA,m8$coef[2:3],NA,m8$R2,m8$adj.R2),
          M9 = c(m9$coef[1],NA,m9$coef[2],NA,m9$coef[3],m9$R2,m9$adj.R2),
          M10 = c(m10$coef[1],NA,NA,m10$coef[2:3],m10$R2,m10$adj.R2),
          M11 = c(m11$coef[1:4],NA,m11$R2,m11$adj.R2),
          M12 = c(m12$coef[1:3],NA,m12$coef[4],m12$R2,m12$adj.R2),
          M13 = c(m13$coef[1:2],NA,m13$coef[3:4],m13$R2,m13$adj.R2),
          M14 = c(m14$coef[1],NA,m14$coef[2:4],m14$R2,m14$adj.R2),
          M15 = c(m15$coef,m15$R2,m15$adj.R2)
          )

rownames(Models_training) <- c("Intercept", "Women Ratio", "Immigrant Ratio", "Income", "Employment Rate", "R-squared", "adj.R-squared")


Models_training %>%
  kable(digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped", "scale_down"))%>%
  add_header_above(c("Computing linear models for Dataset without Region Syddanmark"=16))%>%
  column_spec(14, bold = T, border_left = T, border_right = T)
```


Based on the above table we see that Model 13 is still the best model, because it has the highest value for the adjusted R-Squarred. We now use the new model 13 to predict the number of Aldi stores pr. 10.000 capita in our test_dataset i.e. for Region syddanmark. The predicted value is here calculated by multiplying the coefficients with the observed data for the relevant explanatory variable and then add the intercept coefficient.

```{r Use new model on test_data}

res <- summary(mod <- lm(y_1 ~ x1_1+x3_1+x4_1))
New_m13  <- list(coef=mod$coefficients,R2=res$r.squared,adj.R2=res$adj.r.squared )

Test_data$Test_Prediction <- New_m13$coef[2]*Test_data$Women_ratio + 
  New_m13$coef[3]*Test_data$Indkomst + New_m13$coef[4]*Test_data$Besk + New_m13$coef[1]

Test_data$Test_Residual <- (Test_data$Store_pr_cap - Test_data$Test_Prediction)

Test_data %>%
  select(c("Område", "Store_pr_cap", "Test_Prediction", "Test_Residual"))%>%
  kable(digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))

```


Looking at the above table it is obvious that our model in most cases overestimate the number of Aldi stores pr. 10.000 capita. Also we can calculate the mean model error to be around `r round(mean(Test_data$Test_Residual),3)`. It is not surprising that our average model error in this case is higher than what we observed when we fitted a model on the full dataset (including Region Syddanmark), because there the data from Region Syddanmark had an influence on the coefficients in the model. 


## Calculate the R-squarred using our New Model on the test dataset

We are now going to quantify the how well the new model fit/explains our Test_data, i.e. the dataset containing only Region Syddanmark. To do that we are manually calculating $R^2$ given by the formula:

$$
R^2=1-\frac{R_{SS}}{T_{SS}},
$$
where $R_{SS}=\sum_{i=1}^n \hat{\varepsilon}_i^2$ is the residual variation (Residual sum of Squares) and $T_{SS}=\sum_{i=1}^n(y_i-\overline{y})^2$ is the total variation. So the ratio $\frac{R_{SS}}{T_{SS}}$ tells us how much of the variation in the dataset our model fail to explain, where $1-\frac{R_{SS}}{T_{SS}}$ tells us how much we are able to explain. We calculate these values and collect them in a dataframe:

```{r}
y <- Test_data$Store_pr_cap
Rss <- sum(Test_data$Test_Residual^2)
Tss <- sum((y-mean(y))^2)
R2 <- 1- (Rss/Tss)

performance_model <- data.frame(
  Model13 = c(Rss, Tss, R2)
)

rownames(performance_model) <- c("Rss", "Tss", "R-squared")

performance_model %>%
  kable(digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))

```


As we can see in the above table our $R^2$ is negative. That means that the total variation in the data $T_{SS}$ is less than the variation of the residuals $R_{SS}$, which indicates that our model predicts/explains our date very poorly, actually going against the trend of our data. That also means, that a horizontal line given by the mean of the number of Aldi stores pr. capita better explains our data. When using the mean of the number of Aldi stores (here in Region Syddanmark) as a prediction for the number of Aldi Stores in different areas in Region Syddanmark, we achive an $R^2=0$, which is better than our New_model_13. This is also illustrated in the following Table:  


```{r}
# Mean as prediction

Test_data$Residual_w_mean <- (Test_data$Store_pr_cap - mean(y))

Test_data %>%
  select(c("Område","Test_Residual","Residual_w_mean"))%>%
  kable(digits = 3, "latex", booktabs = T)%>%
  kable_styling(latex_options = c("striped"))%>%
  row_spec(c(5,6,14), bold = T)
```

In the above table we see that the mean as a predicter has a lower residual/error in all the areas except in Haderslev, Kolding and Vejle. So overall the mean as a predicter is actually better than using our model as a predicter. 


\pagebreak

# Conclusion

To shortly conclude on what we have observed in the analysis, we can say that the Aldi stores are more or less evenly distributed across Denmark, but where the concentration of Aldi stores are higher in the bigger cities as Aarhus and Copenhagen. Based on our centrum-stores in Copenhagen and Aarhus, Copenhagen archived the highest concentration (with 11 Aldi stores) when looking at a radius of 5 km to our centrum, but relative to the total amount of Aldi stores in Copenhagen and Aarhus we saw that around $55\%$ was placed within a radius of 5 km in Aarhus, while the percentage in Copenhagen was around $44\%$. We also saw that across all ares in our dataset, the average number of Aldi stores pr. 10.000 capita is around $0.7$, where pr. region Region Sjælland has the highest number of aldi stores pr. 10.000 capita. In the description of our explanatory variables we saw that some variables such as the Men and women Ratio were quite evenly distributed across all areas, whereas some variables such as Income and Immigration ratio contained more variation pr region/area. 

Describing the correlation between the number of Aldi stores and the other explanatory variables revealed no significant correlation with the correlation coefficient from number of Aldi stores and the Labor Force of around $-0.3$ as the most significant. We also noticed in the scatter-plots that the linear regression line didnt explain/fit our data very well, which is also expressed in the R-Squarred/Adj. R-Squarred values in the table computing the best linear model. We otained the highest Adj. R-Squarred model of around $0.08$ which means that the model is able to xplain around $8\%$ of the variation of our data. The more or less poor performance of our model is also expressed in the predictions giving relatively high mean model error/residual for the regions. When predicting the number of Aldi stores in Region Syddanmark using the model 'trained' on our Training_data, we also do quite poorly, actually achieving a negative R-Squarred value, meaning our model goes against the trend of our data, and that our data is better explained with a horizontal line at the average. A factor playing a role in the poor performance of our model might be the fact of a limiting amount of Aldi stores in Denmark.   

\pagebreak

# Repoduce the work

To be able to reproduce my work in this assignment, and to make sure people know wich R environment I have used to do my analysis, i run the following R code as the final remark.

```{r}
sessionInfo()
```


